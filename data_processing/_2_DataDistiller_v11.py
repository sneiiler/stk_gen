"""
åŸºäºGeminiæ¨¡å‹çš„å¤šçº¿ç¨‹æ•°æ®è’¸é¦ç³»ç»Ÿå®ç°ã€‚

This module implements a multi-threaded data distillation system using Gemini models
through OpenAI-compatible interface for better universality and performance.
"""

import os
import sys
from pathlib import Path

root_dir = Path(__file__).parent.parent.parent
print(root_dir)
sys.path.append(str(root_dir))

import json
import re
import threading
import time
from typing import List, Dict, Any, Tuple, Optional
import datetime
from pathlib import Path
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue

from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from tqdm import tqdm
from icecream import ic, install

install()

from utils.misc_utils import get_data_dir, get_project_root
from utils.prompt_template import get_prompt_template
from data_models.sft_data_models import (
    ClusterInfo,
    RawConstellationDataModel,
    SatelliteClusterOutput,
    ShareGPTFormat,
    ShareGPTMessage,
)
from dotenv import load_dotenv

env_path = get_project_root() / ".env"
load_dotenv(env_path)

# è·å–Gemini APIé…ç½®
api_base_gemini = os.getenv("GEMINI_API_BASE")
api_key_gemini = os.getenv("GEMINI_API_KEY_8")

print(f"Gemini APIé…ç½®: {'âœ“' if api_key_gemini and api_base_gemini else 'âœ—'}")

if not api_key_gemini or not api_base_gemini:
    raise ValueError(
        "æœªæ‰¾åˆ°Gemini APIé…ç½®ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ GEMINI_API_KEY å’Œ GEMINI_API_BASE"
    )


class ThreadSafeWriter:
    """çº¿ç¨‹å®‰å…¨çš„æ–‡ä»¶å†™å…¥å™¨"""

    def __init__(self, file_path: Path):
        self.file_path = file_path
        self.lock = threading.Lock()
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.file_path.parent.mkdir(parents=True, exist_ok=True)

    def write_line(self, data: str):
        """çº¿ç¨‹å®‰å…¨åœ°å†™å…¥ä¸€è¡Œæ•°æ®"""
        with self.lock:
            with open(self.file_path, "a", encoding="utf-8") as f:
                f.write(data + "\n")
                f.flush()


class RateLimiter:
    """ç®€å•çš„é€Ÿç‡é™åˆ¶å™¨"""

    def __init__(self, requests_per_minute: int = 60):
        self.requests_per_minute = requests_per_minute
        self.min_interval = 60.0 / requests_per_minute
        self.last_request_time = 0
        self.lock = threading.Lock()

    def wait_if_needed(self):
        """å¦‚æœéœ€è¦çš„è¯ç­‰å¾…ä»¥æ»¡è¶³é€Ÿç‡é™åˆ¶"""
        with self.lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            if time_since_last < self.min_interval:
                sleep_time = self.min_interval - time_since_last
                time.sleep(sleep_time)
            self.last_request_time = time.time()


class DataDistiller:
    """åŸºäºGeminiæ¨¡å‹çš„å¤šçº¿ç¨‹æ•°æ®è’¸é¦å™¨ç±»ã€‚

    ä¸»è¦åŠŸèƒ½ï¼š
    - å¤šçº¿ç¨‹å¹¶å‘å¤„ç†æå‡æ•ˆç‡
    - æ”¯æŒæµå¼å“åº”è·å–æ€è€ƒè¿‡ç¨‹
    - æ™ºèƒ½é€Ÿç‡é™åˆ¶é¿å…APIé™åˆ¶
    - çº¿ç¨‹å®‰å…¨çš„æ–‡ä»¶å†™å…¥
    - ä¸“é—¨é’ˆå¯¹Geminiæ¨¡å‹ä¼˜åŒ–

    Attributes:
        client: OpenAIå…¼å®¹å®¢æˆ·ç«¯
        prompt_template: æç¤ºæ¨¡æ¿
        output_parser: è¾“å‡ºè§£æå™¨
        rate_limiter: é€Ÿç‡é™åˆ¶å™¨
    """

    def __init__(
        self,
        model_name: str = "",
        temperature: float = 0.1,
        proxy: Optional[str] = None,
        requests_per_minute: int = 60,
        max_workers: int = 5,
        reasoning_effort: str = "high",  # low, medium, high
    ):
        """åˆå§‹åŒ–æ•°æ®è’¸é¦å™¨ã€‚

        Args:
            model_name: Geminiæ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
            proxy: ä»£ç†è®¾ç½®
            requests_per_minute: æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            reasoning_effort: æ¨ç†å¼ºåº¦ (low, medium, high)
        """

        self.model_name = model_name
        self.temperature = temperature
        self.max_workers = max_workers
        self.reasoning_effort = reasoning_effort
        self.rate_limiter = RateLimiter(requests_per_minute)

        # åˆå§‹åŒ–OpenAIå…¼å®¹å®¢æˆ·ç«¯
        if proxy is not None:
            import httpx

            httpx_client = httpx.Client(proxy=proxy)
            print(f"ä½¿ç”¨ä»£ç†: {proxy}")
            self.client = OpenAI(
                api_key=api_key_gemini,
                base_url=api_base_gemini,
                http_client=httpx_client,
            )
        else:
            self.client = OpenAI(api_key=api_key_gemini, base_url=api_base_gemini)

        # è·å–promptæ¨¡æ¿
        self.prompt_template = ChatPromptTemplate.from_template(
            template=get_prompt_template("latest")
        )

        # åˆ›å»ºè¾“å‡ºè§£æå™¨
        self.output_parser = PydanticOutputParser(
            pydantic_object=SatelliteClusterOutput
        )

        print(f"Geminiæ•°æ®è’¸é¦å™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"- æ¨¡å‹: {self.model_name}")
        print(f"- æ¨ç†å¼ºåº¦: {self.reasoning_effort}")
        print(f"- æœ€å¤§å¹¶å‘: {self.max_workers}")
        print(f"- è¯·æ±‚é¢‘ç‡: {requests_per_minute}/åˆ†é’Ÿ")

    def _extract_reasoning_and_content(self, response_stream) -> Tuple[str, str]:
        """ä»æµå¼å“åº”ä¸­æå–æ€è€ƒè¿‡ç¨‹å’Œå†…å®¹"""
        reasoning_content = ""
        content = ""

        try:
            for chunk in response_stream:
                if not chunk.choices:
                    continue

                delta = chunk.choices[0].delta


                # è·å–æ€è€ƒè¿‡ç¨‹ï¼ˆreasoningï¼‰- Gemini 2.5ä¸æ”¯æŒ
                if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                    reasoning_content += delta.reasoning_content

                # è·å–æ­£å¸¸å†…å®¹
                if hasattr(delta, "content") and delta.content:
                    content += delta.content
                    print(f"è·å–åˆ°æ–°çš„contentï¼Œé•¿åº¦: {len(delta.content)}")



                # æ£€æŸ¥æ˜¯å¦æ˜¯Geminiçš„æ€ç»´é“¾å†…å®¹ï¼ˆåŒ…å«åœ¨<thought>æ ‡ç­¾ä¸­ï¼‰
                thought_match = re.search(
                    r"<thought>(.*?)</thought>", content, re.DOTALL
                )
                if thought_match:
                    thought_content = thought_match.group(1)
                    reasoning_content += thought_content
                    # ä»contentä¸­ç§»é™¤thoughtéƒ¨åˆ†ï¼Œä¿æŒå†…å®¹å¹²å‡€
                    content = re.sub(
                        r"<thought>.*?</thought>", "", content, flags=re.DOTALL
                    )

        except Exception as e:
            print(f"å¤„ç†æµå¼å“åº”æ—¶å‡ºé”™: {str(e)}")

        return reasoning_content.strip(), content.strip()

    def generate_distill_result(
        self, data: Dict[str, Any], sample_index: int = 0
    ) -> Tuple[Optional[SatelliteClusterOutput], str, str]:
        """ç”Ÿæˆè’¸é¦ç»“æœã€‚

        Args:
            data: è¾“å…¥æ•°æ®
            sample_index: æ ·æœ¬ç´¢å¼•ï¼Œç”¨äºé”™è¯¯è¿½è¸ª

        Returns:
            (ç»“æœå¯¹è±¡, ç³»ç»Ÿæç¤º, é”™è¯¯ä¿¡æ¯)çš„å…ƒç»„
        """
        try:
            # åº”ç”¨é€Ÿç‡é™åˆ¶
            self.rate_limiter.wait_if_needed()

            # æ ¼å¼åŒ–è¾“å…¥æ•°æ®
            user_content = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
            user_content = (
                user_content.replace("\n", " ").replace("\r", " ").replace("\t", " ")
            )
            user_content = re.sub(r"\s+", " ", user_content)

            # è·å–æ ¼å¼è¯´æ˜
            input_instructions = RawConstellationDataModel.model_json_schema()
            format_instructions = self.output_parser.get_format_instructions()

            # ç”Ÿæˆå®Œæ•´æç¤º
            system_prompt = self.prompt_template.format(
                input_instructions=input_instructions,
                output_format_instructions=format_instructions,
            )
            system_prompt = (
                system_prompt.replace("\n", " ").replace("\r", " ").replace("\t", " ")
            )
            system_prompt = re.sub(r"\s+", " ", system_prompt)

            messages = [
                {"role": "system", "content": system_prompt.strip()},
                {"role": "user", "content": user_content.strip()},
            ]

            # åˆ›å»ºæµå¼è¯·æ±‚ - é’ˆå¯¹Geminiä¼˜åŒ–
            response = self.client.chat.completions.create(
                model=self.model_name,
                # reasoning_effort=self.reasoning_effort,  # Gemini 2.5æ”¯æŒ # type: ignore
                messages=messages,  # type: ignore
                stream=True,
                temperature=self.temperature,
                extra_body={
                    "extra_body": {
                        "google": {
                            "thinking_config": {
                                "thinking_budget": 12000,
                                "include_thoughts": True,
                            }
                        }
                    }
                },
            )

            # æå–æ€è€ƒè¿‡ç¨‹å’Œå†…å®¹
            reasoning_content, content = self._extract_reasoning_and_content(response)

            print(f"æ ·æœ¬ {sample_index} - æ€è€ƒè¿‡ç¨‹é•¿åº¦: {len(reasoning_content)}")
            print(f"æ ·æœ¬ {sample_index} - å†…å®¹é•¿åº¦: {len(content)}")

            if not content.strip():
                return None, system_prompt, f"APIè¿”å›å†…å®¹ä¸ºç©º (æ ·æœ¬ {sample_index})"

            # è§£æJSONè¾“å‡º
            try:
                # ç›´æ¥ä½¿ç”¨output_parserè§£æ
                parsed_output = self.output_parser.parse(content)

                # æå–æ€ç»´é“¾
                cot = parsed_output.chain_of_thought
                # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œå°†å…¶æ·»åŠ åˆ°æ€ç»´é“¾å‰é¢
                if reasoning_content:
                    cot = f"{reasoning_content}\næ€è€ƒè¿‡ç¨‹æ€»ç»“:\n{cot}"

                result = SatelliteClusterOutput(
                    chain_of_thought=cot,
                    clusters=[
                        ClusterInfo(
                            cluster_id=cluster.cluster_id,
                            master=cluster.master,
                            sats=cluster.sats,
                            targets=cluster.targets,
                        )
                        for cluster in parsed_output.clusters
                    ],
                )
                return result, system_prompt, ""

            except Exception as e:
                error_msg = f"è§£æè¾“å‡ºå¤±è´¥ (æ ·æœ¬ {sample_index}): {str(e)}"
                print(error_msg)
                print(f"åŸå§‹å†…å®¹: {content[:500]}...")
                print(f"æ€è€ƒè¿‡ç¨‹: {reasoning_content[:200]}...")
                return None, system_prompt, error_msg

        except Exception as e:
            error_msg = f"APIè°ƒç”¨å¤±è´¥ (æ ·æœ¬ {sample_index}): {str(e)}"
            print(error_msg)
            return None, "", error_msg

    def create_sharegpt_format(
        self, instruction: str, input_data: str, output_data: str
    ) -> ShareGPTFormat:
        """åˆ›å»ºShareGPTæ ¼å¼çš„è®­ç»ƒæ•°æ®ã€‚

        Args:
            instruction: æŒ‡ä»¤å†…å®¹
            input_data: è¾“å…¥æ•°æ®
            output_data: è¾“å‡ºæ•°æ®

        Returns:
            ShareGPTæ ¼å¼çš„æ•°æ®
        """
        return ShareGPTFormat(
            messages=[
                ShareGPTMessage(role="system", content=instruction),
                ShareGPTMessage(role="user", content=input_data),
                ShareGPTMessage(role="assistant", content=output_data),
            ]
        )

    def process_single_item(
        self,
        item_data: Tuple[int, Dict[str, Any]],
        writer: ThreadSafeWriter,
        stats_queue: Queue,
    ) -> None:
        """å¤„ç†å•ä¸ªæ•°æ®é¡¹ï¼ˆåœ¨çº¿ç¨‹ä¸­è°ƒç”¨ï¼‰"""
        index, data = item_data

        try:
            # å¤„ç†æ•°æ®
            result, system_prompt, error_msg = self.generate_distill_result(data, index)

            if result is None:
                # è®°å½•é”™è¯¯
                error_data = {
                    "error": error_msg,
                    "data": json.dumps(data, ensure_ascii=False, separators=(",", ":")),
                    "sample_index": index,
                    "timestamp": datetime.datetime.now().isoformat(),
                    "model": self.model_name,
                }
                writer.write_line(
                    f"ERROR: {json.dumps(error_data, ensure_ascii=False)}"
                )
                stats_queue.put("failed")
                return

            # æ„é€ ShareGPTæ ¼å¼çš„è®­ç»ƒæ•°æ®
            input_str = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
            output_str = result.to_think_json()

            sharegpt_data = self.create_sharegpt_format(
                instruction=system_prompt,
                input_data=input_str,
                output_data=output_str,
            )

            # å†™å…¥æˆåŠŸç»“æœ
            writer.write_line(sharegpt_data.model_dump_json())
            stats_queue.put("success")

        except Exception as e:
            error_data = {
                "error": f"å¤„ç†å¼‚å¸¸: {str(e)}",
                "sample_index": index,
                "timestamp": datetime.datetime.now().isoformat(),
                "model": self.model_name,
            }
            writer.write_line(f"ERROR: {json.dumps(error_data, ensure_ascii=False)}")
            stats_queue.put("failed")

    def process_batch_multithread(
        self,
        batch_data: List[Dict[str, Any]],
        output_file: Path,
    ) -> None:
        """å¤šçº¿ç¨‹æ‰¹é‡å¤„ç†æ•°æ®å¹¶å®æ—¶ä¿å­˜ã€‚

        Args:
            batch_data: åŒ…å«ä»»åŠ¡æè¿°å’Œè¾“å…¥æ•°æ®çš„åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # åˆå§‹åŒ–çº¿ç¨‹å®‰å…¨çš„å†™å…¥å™¨
        writer = ThreadSafeWriter(output_file)

        # ç»Ÿè®¡ä¿¡æ¯é˜Ÿåˆ—
        stats_queue = Queue()

        # åˆ›å»ºå¸¦ç´¢å¼•çš„æ•°æ®åˆ—è¡¨
        indexed_data = list(enumerate(batch_data))

        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
        print(f"å¼€å§‹å¤šçº¿ç¨‹å¤„ç†ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(
                    self.process_single_item, item, writer, stats_queue
                ): item[0]
                for item in indexed_data
            }

            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(batch_data), desc="å¤„ç†æ•°æ®æ‰¹æ¬¡") as pbar:
                for future in as_completed(future_to_index):
                    try:
                        future.result()  # è·å–ç»“æœï¼Œå¦‚æœæœ‰å¼‚å¸¸ä¼šæŠ›å‡º
                    except Exception as e:
                        index = future_to_index[future]
                        print(f"çº¿ç¨‹å¤„ç†æ ·æœ¬ {index} æ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
                    finally:
                        pbar.update(1)

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        stats = {"total": len(batch_data), "success": 0, "failed": 0}
        while not stats_queue.empty():
            result = stats_queue.get()
            if result == "success":
                stats["success"] += 1
            elif result == "failed":
                stats["failed"] += 1

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"å¤šçº¿ç¨‹å¤„ç†å®Œæˆç»Ÿè®¡: {stats}")
        print(f"æˆåŠŸç‡: {stats['success']/stats['total']*100:.2f}%")


def load_json_data(file_path: Path) -> List[Dict[str, Any]]:
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®ã€‚

    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„

    Returns:
        åŠ è½½çš„æ•°æ®åˆ—è¡¨
    """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


def main():
    """ä¸»å‡½æ•°ã€‚"""
    # ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
    input_file = (
        get_data_dir() / "mock_satellite_observation_data_20250630_090758_v11.json"
    )
    batch_data = load_json_data(input_file)

    print(f"åŠ è½½äº† {len(batch_data)} ä¸ªæ•°æ®æ ·æœ¬")

    proxy = "socks5://127.0.0.1:1089"
    model_name = "gemini-2.5-pro"  # æˆ– "gemini-2.5-pro" å¦‚éœ€æ›´é«˜è´¨é‡

    # åˆå§‹åŒ–Geminiè’¸é¦å™¨
    distiller = DataDistiller(
        model_name=model_name,  # æˆ– "gemini-2.5-pro" å¦‚éœ€æ›´é«˜è´¨é‡
        temperature=0.1,
        proxy=proxy,  # "socks5://127.0.0.1:1089" å¦‚æœéœ€è¦ä»£ç†
        requests_per_minute=60,  # æ ¹æ®ä½ çš„APIé™åˆ¶è°ƒæ•´
        max_workers=6,  # å»ºè®®ä»4å¼€å§‹ï¼ŒæˆåŠŸåå¯ä»¥å¢åŠ åˆ°6-8
        reasoning_effort="high",  # low/medium/high, æ§åˆ¶æ€è€ƒæ·±åº¦
    )

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = (
        get_data_dir()
        / f"training_data_sharegpt_{model_name}_{now}_{str(input_file)[-10:-5]}.jsonl"
    )

    print(f"\nğŸ“‹ å¤„ç†é…ç½®:")
    print(f"- æ¨¡å‹: {distiller.model_name}")
    print(f"- æ¨ç†å¼ºåº¦: {distiller.reasoning_effort}")
    print(f"- å¹¶å‘çº¿ç¨‹: {distiller.max_workers}")
    print(f"- è¯·æ±‚é¢‘ç‡: {distiller.rate_limiter.requests_per_minute}/åˆ†é’Ÿ")
    print(f"- è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"- æ•°æ®é‡: {len(batch_data)} ä¸ªæ ·æœ¬")

    # å¤šçº¿ç¨‹å¤„ç†æ•°æ®å¹¶å®æ—¶ä¿å­˜
    start_time = time.time()
    distiller.process_batch_multithread(batch_data, output_file)
    end_time = time.time()

    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"ğŸ“Š å¹³å‡æ¯ä¸ªæ ·æœ¬è€—æ—¶: {(end_time - start_time) / len(batch_data):.2f} ç§’")

    # æä¾›ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print(f"1. å¦‚æœæˆåŠŸç‡è¾ƒé«˜ï¼Œå¯ä»¥å°† max_workers å¢åŠ åˆ° 6-8")
    print(f"2. å¦‚æœé‡åˆ°429é”™è¯¯ï¼Œé™ä½ requests_per_minute æˆ– max_workers")
    print(f"3. å¦‚æœéœ€è¦æ›´é«˜è´¨é‡çš„æ€è€ƒï¼Œå°† reasoning_effort æ”¹ä¸º 'high'")
    print(f"4. å¦‚æœéœ€è¦æ›´é«˜æ•´ä½“è´¨é‡ï¼Œå¯ä»¥å°è¯• 'gemini-2.5-pro' æ¨¡å‹")


if __name__ == "__main__":
    main()
